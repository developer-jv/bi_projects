{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9715cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.avro.functions import from_avro\n",
    "from pyspark.sql.types import BinaryType\n",
    "import json, urllib.request\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .remote(\"sc://localhost:15002\")\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\")\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"/opt/warehouse\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Ejecutores (contenedores) -> Kafka vía host.docker.internal\n",
    "KAFKA_BOOTSTRAP = \"host.docker.internal:9094\"\n",
    "\n",
    "# Cliente (tu notebook) -> Schema Registry en el host\n",
    "SCHEMA_REGISTRY = \"http://localhost:8081\"\n",
    "\n",
    "TOPIC = \"erp_avro.erp.orders\"\n",
    "SCHEMA_SUBJECT = f\"{TOPIC}-value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c0de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Leer crudo de Kafka (ya te funciona)\n",
    "df_raw = (\n",
    "    spark.read.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "    .option(\"subscribe\", TOPIC)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"endingOffsets\", \"latest\")\n",
    "    .option(\"includeHeaders\", \"true\")  # opcional\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# 2) Traer schema Avro del Registry (desde el host)\n",
    "def fetch_latest_schema(registry_url, subject):\n",
    "    with urllib.request.urlopen(f\"{registry_url.rstrip('/')}/subjects/{subject}/versions/latest\") as resp:\n",
    "        meta = json.load(resp)\n",
    "        schema_str = meta[\"schema\"]\n",
    "        # normaliza por si viene doblemente escapado\n",
    "        try:\n",
    "            json.loads(schema_str)\n",
    "            return schema_str\n",
    "        except json.JSONDecodeError:\n",
    "            return json.dumps(json.loads(schema_str))\n",
    "\n",
    "value_schema = fetch_latest_schema(SCHEMA_REGISTRY, SCHEMA_SUBJECT)\n",
    "\n",
    "# 3) Decodificar Avro con framing Confluent (quita 5 bytes)\n",
    "drop_header_udf = F.udf(lambda b: (b[5:] if b and len(b) >= 5 else b), BinaryType())\n",
    "\n",
    "df_decoded = (\n",
    "    df_raw\n",
    "    .select(drop_header_udf(F.col(\"value\")).alias(\"payload\"))\n",
    "    .select(from_avro(F.col(\"payload\"), value_schema).alias(\"data\"))\n",
    "    .select(\"data.*\")\n",
    ")\n",
    "\n",
    "df_decoded.printSchema()\n",
    "df_decoded.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de41b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_events = (\n",
    "    df_decoded\n",
    "    .withColumn(\"op\", F.col(\"op\"))\n",
    "    .withColumn(\"is_snapshot\", (F.col(\"op\") == F.lit(\"r\")))\n",
    "    .selectExpr(\n",
    "        \"op\",\n",
    "        \"is_snapshot\",\n",
    "        \"source.ts_ms as source_ts_ms\",\n",
    "        \"source.db as src_db\",\n",
    "        \"source.table as src_table\",\n",
    "        \"transaction.id as tx_id\",\n",
    "        # Elegimos los campos desde after/before según op\n",
    "        \"CASE WHEN op in ('c','r','u') THEN after.order_id ELSE before.order_id END       as order_id\",\n",
    "        \"CASE WHEN op in ('c','r','u') THEN after.customer_id ELSE before.customer_id END as customer_id\",\n",
    "        \"CASE WHEN op in ('c','r','u') THEN after.order_ts ELSE before.order_ts END       as order_ts\",\n",
    "        \"CASE WHEN op in ('c','r','u') THEN after.status ELSE before.status END           as status\",\n",
    "        \"CASE WHEN op in ('c','r','u') THEN after.amount ELSE before.amount END           as amount\",\n",
    "        \"CASE WHEN op in ('c','r','u') THEN after.currency ELSE before.currency END       as currency\",\n",
    "        \"CASE WHEN op in ('c','r','u') THEN after.created_at ELSE before.created_at END   as created_at_str\",\n",
    "        \"CASE WHEN op in ('c','r','u') THEN after.updated_at ELSE before.updated_at END   as updated_at_str\"\n",
    "    )\n",
    "    # Si quieres, convierte created_at/updated_at a timestamp\n",
    "    .withColumn(\"created_at\", F.to_timestamp(\"created_at_str\"))\n",
    "    .withColumn(\"updated_at\", F.to_timestamp(\"updated_at_str\"))\n",
    "    .drop(\"created_at_str\", \"updated_at_str\")\n",
    ")\n",
    "\n",
    "df_events.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2dc56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6631948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
