{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7347b772",
   "metadata": {},
   "source": [
    "\n",
    "# 01 — Leer y explorar tablas **Apache Iceberg** (Spark Connect)\n",
    "\n",
    "Este notebook te guía paso a paso para:\n",
    "1) Conectarte a tu **Spark Connect Server**.  \n",
    "2) Explorar el catálogo/warehouse de **Iceberg**.  \n",
    "3) **Leer** una tabla (`orders_iceberg`) ya creada por tu otro job.  \n",
    "4) Hacer un **perfil rápido** y, si es posible, mirar **snapshots** y **time travel**.\n",
    "\n",
    "> Requisitos previos: tu Spark Connect Server debe tener cargado `iceberg-spark-runtime-4.0_2.13` y montar el warehouse donde escribe tu otro job, por ejemplo:  \n",
    "> `/opt/tables_apache_iceberg/warehouse` (mapeado desde `demo_projects/01_CDC_GCP/tables_apache_iceberg`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5c499e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECT_URL        : sc://localhost:15002\n",
      "ICEBERG_CATALOG    : lh\n",
      "ICEBERG_WAREHOUSE  : /opt/tables_apache_iceberg/warehouse\n",
      "TABLE FQN (catalog): lh.erp.orders_iceberg\n",
      "TABLE PATH         : /opt/tables_apache_iceberg/warehouse/erp/orders_iceberg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Parámetros a tu medida ===\n",
    "CONNECT_URL = \"sc://localhost:15002\"  # URL de Spark Connect Server\n",
    "\n",
    "# Catálogo del warehouse del job de escritura (configurado en el servidor)\n",
    "ICEBERG_CATALOG = \"lh\"                 # catálogo configurado en el server\n",
    "ICEBERG_WAREHOUSE = \"/opt/tables_apache_iceberg/warehouse\"  # ruta dentro del contenedor del server\n",
    "\n",
    "# Identificadores de la tabla final\n",
    "DB = \"erp\"\n",
    "TABLE = \"orders_iceberg\"\n",
    "\n",
    "# (Alternativa) Lectura por PATH directo (si no usas catálogo)\n",
    "TABLE_PATH = f\"{ICEBERG_WAREHOUSE}/{DB}/{TABLE}\"\n",
    "\n",
    "print(\"CONNECT_URL        :\", CONNECT_URL)\n",
    "print(\"ICEBERG_CATALOG    :\", ICEBERG_CATALOG)\n",
    "print(\"ICEBERG_WAREHOUSE  :\", ICEBERG_WAREHOUSE)\n",
    "print(\"TABLE FQN (catalog):\", f\"{ICEBERG_CATALOG}.{DB}.{TABLE}\")\n",
    "print(\"TABLE PATH         :\", TABLE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18111664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .remote(CONNECT_URL)\n",
    "    # Estas configs suelen estar en el servidor; aquí son redundantes pero no estorban.\n",
    "    .config(f\"spark.sql.catalog.{ICEBERG_CATALOG}\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(f\"spark.sql.catalog.{ICEBERG_CATALOG}.type\", \"hadoop\")\n",
    "    .config(f\"spark.sql.catalog.{ICEBERG_CATALOG}.warehouse\", ICEBERG_WAREHOUSE)\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "171d412f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespaces en catálogo 'lh':\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|erp      |\n",
      "+---------+\n",
      "\n",
      "Tablas en lh.erp:\n",
      "+---------+--------------+-----------+\n",
      "|namespace|tableName     |isTemporary|\n",
      "+---------+--------------+-----------+\n",
      "|erp      |orders_iceberg|false      |\n",
      "+---------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Intentamos listar namespaces y tablas del catálogo de Iceberg\n",
    "try:\n",
    "    print(f\"Namespaces en catálogo '{ICEBERG_CATALOG}':\")\n",
    "    spark.sql(f\"SHOW NAMESPACES IN {ICEBERG_CATALOG}\").show(truncate=False)\n",
    "    print(f\"Tablas en {ICEBERG_CATALOG}.{DB}:\")\n",
    "    spark.sql(f\"SHOW TABLES IN {ICEBERG_CATALOG}.{DB}\").show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(\"No se pudo listar namespaces/tablas vía catálogo:\", e)\n",
    "    print(\"Puedes seguir leyendo por PATH si el catálogo no está disponible.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aa79d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intentando leer por catálogo: lh.erp.orders_iceberg\n",
      "✓ Leído por catálogo.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def load_iceberg_table(spark) -> DataFrame:\n",
    "    \"\"\"Intenta cargar la tabla usando el catálogo; si falla, prueba por PATH.\"\"\"\n",
    "    fqn = f\"{ICEBERG_CATALOG}.{DB}.{TABLE}\"\n",
    "    # 1) Probar catálogo\n",
    "    try:\n",
    "        print(f\"Intentando leer por catálogo: {fqn}\")\n",
    "        df = spark.table(fqn)\n",
    "        # Forzamos una acción ligera para validar\n",
    "        df.limit(1).collect()\n",
    "        print(\"✓ Leído por catálogo.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"No se pudo leer por catálogo:\", e)\n",
    "\n",
    "    # 2) Probar por PATH\n",
    "    try:\n",
    "        print(f\"Intentando leer por PATH: {TABLE_PATH}\")\n",
    "        df = spark.read.format(\"iceberg\").load(TABLE_PATH)\n",
    "        df.limit(1).collect()\n",
    "        print(\"✓ Leído por PATH.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"No se pudo leer por PATH:\", e)\n",
    "        raise RuntimeError(\"No se pudo cargar la tabla por catálogo ni por PATH. Revisa mounts/config.\") from e\n",
    "\n",
    "df_ice = load_iceberg_table(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfe5c7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- amount: decimal(12,2) (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n",
      "Total de filas en la tabla: 4\n",
      "+--------+-----------+------+-------+-------------------+\n",
      "|order_id|customer_id|amount|status |ts                 |\n",
      "+--------+-----------+------+-------+-------------------+\n",
      "|1       |1          |120.50|CREATED|2025-09-28 21:18:12|\n",
      "|2       |1          |89.90 |PAID   |2025-09-28 21:18:12|\n",
      "|3       |2          |45.00 |CREATED|2025-09-28 21:18:12|\n",
      "|4       |3          |320.00|SHIPPED|2025-09-28 21:18:12|\n",
      "+--------+-----------+------+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_ice.printSchema()\n",
    "\n",
    "try:\n",
    "    cnt = df_ice.count()\n",
    "    print(\"Total de filas en la tabla:\", cnt)\n",
    "except Exception as e:\n",
    "    print(\"No se pudo contar filas (posible tabla muy grande). Error:\", e)\n",
    "\n",
    "df_ice.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "cols = df_ice.columns\n",
    "print(\"Columnas:\", cols)\n",
    "\n",
    "# Conteos por status/currency si existen\n",
    "if \"status\" in cols:\n",
    "    print(\"\\nConteo por status:\")\n",
    "    df_ice.groupBy(\"status\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "\n",
    "if \"currency\" in cols:\n",
    "    print(\"\\nConteo por currency:\")\n",
    "    df_ice.groupBy(\"currency\").count().orderBy(F.desc(\"count\")).show(truncate=False)\n",
    "\n",
    "# Estadísticos de 'amount' si existe\n",
    "if \"amount\" in cols:\n",
    "    print(\"\\nEstadísticos de 'amount':\")\n",
    "    df_ice.select(\"amount\").summary().show(truncate=False)\n",
    "\n",
    "# Nulos por columna (rápido en muestras grandes: calcula sobre todo el DF, cuidado con costos)\n",
    "print(\"\\nNulos por columna (0 = sin nulos):\")\n",
    "nulls = [F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in cols]\n",
    "df_ice.agg(*nulls).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534a4f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "fqn = f\"{ICEBERG_CATALOG}.{DB}.{TABLE}\"\n",
    "def try_show(sql):\n",
    "    try:\n",
    "        spark.sql(sql).show(truncate=False)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo ejecutar: {sql}\\n ->\", e)\n",
    "        return False\n",
    "\n",
    "print(f\"Intentando metadatos ACID en: {fqn}\")\n",
    "_ = try_show(f\"SELECT * FROM {fqn}.snapshots ORDER BY committed_at DESC\")\n",
    "_ = try_show(f\"SELECT * FROM {fqn}.history ORDER BY made_current_at DESC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe93599",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Intentamos obtener el último snapshot_id del catálogo y leer esa versión explícitamente\n",
    "try:\n",
    "    snaps = spark.sql(f\"SELECT snapshot_id FROM {ICEBERG_CATALOG}.{DB}.{TABLE}.snapshots ORDER BY committed_at DESC\")\n",
    "    snap = snaps.limit(1).collect()[0][\"snapshot_id\"]\n",
    "    print(\"Último snapshot_id:\", snap)\n",
    "\n",
    "    df_tt = (\n",
    "        spark.read.format(\"iceberg\")\n",
    "        .option(\"snapshot-id\", str(snap))\n",
    "        .load(f\"{ICEBERG_CATALOG}.{DB}.{TABLE}\")\n",
    "    )\n",
    "    print(\"Time travel (10 filas):\")\n",
    "    df_tt.show(10, truncate=False)\n",
    "except Exception as e:\n",
    "    print(\"No fue posible realizar time travel por catálogo. Intentando por PATH...\")\n",
    "    try:\n",
    "        # Si sabemos un snapshot_id manualmente, también sirve con PATH\n",
    "        # df_tt = (spark.read.format(\"iceberg\").option(\"snapshot-id\", \"<ID>\").load(TABLE_PATH))\n",
    "        # Aquí solo dejamos la mecánica impresa.\n",
    "        print(\"Para usar time travel por PATH, necesitas un snapshot_id conocido.\")\n",
    "    except Exception as e2:\n",
    "        print(\"No se pudo realizar time travel:\", e2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac52dfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Top clientes por monto\n",
    "if set([\"customer_id\",\"amount\"]).issubset(df_ice.columns):\n",
    "    print(\"Top 10 clientes por monto:\")\n",
    "    (df_ice.groupBy(\"customer_id\")\n",
    "          .agg(F.count(\"*\").alias(\"orders\"), F.sum(\"amount\").alias(\"total_amount\"))\n",
    "          .orderBy(F.desc(\"total_amount\"))\n",
    "          .show(10, truncate=False))\n",
    "\n",
    "# Actividad diaria por order_ts\n",
    "if \"order_ts\" in df_ice.columns:\n",
    "    print(\"\\nActividad diaria (conteo y monto):\")\n",
    "    daily = (df_ice\n",
    "             .withColumn(\"order_date\", F.to_date(\"order_ts\"))\n",
    "             .groupBy(\"order_date\")\n",
    "             .agg(F.count(\"*\").alias(\"orders\"), F.sum(\"amount\").alias(\"amount_sum\"))\n",
    "             .orderBy(\"order_date\"))\n",
    "    daily.show(20, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
