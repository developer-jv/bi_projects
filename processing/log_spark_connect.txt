[38;5;6mspark [38;5;5m05:01:08.85 [0m[38;5;2mINFO [0m ==> 
[38;5;6mspark [38;5;5m05:01:08.85 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
[38;5;6mspark [38;5;5m05:01:08.85 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
[38;5;6mspark [38;5;5m05:01:08.85 [0m[38;5;2mINFO [0m ==> Did you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami/ for more information.
[38;5;6mspark [38;5;5m05:01:08.86 [0m[38;5;2mINFO [0m ==> 

rsync from spark://spark-master:7077
/opt/bitnami/spark/sbin/spark-daemon.sh: line 181: rsync: command not found
starting org.apache.spark.sql.connect.service.SparkConnectServer, logging to /opt/bitnami/spark/logs/spark-spark-org.apache.spark.sql.connect.service.SparkConnectServer-1-f6f0f97b91c5.out
Spark Command: /opt/bitnami/java/bin/java -cp hive-jackson/*:/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/slf4j-api-2.0.16.jar:/opt/bitnami/spark/jars/* -Xmx1g --add-exports java.base/sun.nio.ch=ALL-UNNAMED -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true -Dderby.connection.requireAuthentication=false org.apache.spark.deploy.SparkSubmit --master spark://spark-master:7077 --conf spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.local.warehouse=/opt/warehouse --conf spark.driver.extraJavaOptions=--add-exports java.base/sun.nio.ch=ALL-UNNAMED --conf spark.sql.catalog.local.type=hadoop --class org.apache.spark.sql.connect.service.SparkConnectServer --name Spark Connect server --packages org.apache.spark:spark-connect_2.13:4.0.0,org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.0,org.apache.spark:spark-avro_2.13:4.0.0,org.apache.iceberg:iceberg-spark-runtime-4.0_2.13:1.10.0 spark-internal
========================================
WARNING: Using incubator modules: jdk.incubator.vector
:: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /opt/bitnami/spark/.ivy2.5.2/cache
The jars for the packages stored in: /opt/bitnami/spark/.ivy2.5.2/jars
org.apache.spark#spark-connect_2.13 added as a dependency
org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
org.apache.spark#spark-avro_2.13 added as a dependency
org.apache.iceberg#iceberg-spark-runtime-4.0_2.13 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-b54493f2-4165-4bb1-99d9-e8172b1e5bee;1.0
	confs: [default]
	found org.apache.spark#spark-connect_2.13;4.0.0 in central
	found org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 in central
	found jakarta.servlet#jakarta.servlet-api;5.0.0 in central
	found javax.servlet#javax.servlet-api;4.0.1 in central
	found org.spark-project.spark#unused;1.0.0 in central
	found org.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 in central
	found org.apache.kafka#kafka-clients;3.9.0 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.7 in central
	found org.slf4j#slf4j-api;2.0.16 in central
	found org.apache.hadoop#hadoop-client-runtime;3.4.1 in central
	found org.apache.hadoop#hadoop-client-api;3.4.1 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.12.0 in central
	found org.apache.spark#spark-avro_2.13;4.0.0 in central
	found org.tukaani#xz;1.10 in central
	found org.apache.iceberg#iceberg-spark-runtime-4.0_2.13;1.10.0 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-connect_2.13/4.0.0/spark-connect_2.13-4.0.0.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-connect_2.13;4.0.0!spark-connect_2.13.jar (780ms)
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.13/4.0.0/spark-sql-kafka-0-10_2.13-4.0.0.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0!spark-sql-kafka-0-10_2.13.jar (75ms)
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.13/4.0.0/spark-avro_2.13-4.0.0.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-avro_2.13;4.0.0!spark-avro_2.13.jar (89ms)
downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-4.0_2.13/1.10.0/iceberg-spark-runtime-4.0_2.13-1.10.0.jar ...
	[SUCCESSFUL ] org.apache.iceberg#iceberg-spark-runtime-4.0_2.13;1.10.0!iceberg-spark-runtime-4.0_2.13.jar (2593ms)
downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-parallel-collections_2.13/1.2.0/scala-parallel-collections_2.13-1.2.0.jar ...
	[SUCCESSFUL ] org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0!scala-parallel-collections_2.13.jar (122ms)
downloading https://repo1.maven.org/maven2/jakarta/servlet/jakarta.servlet-api/5.0.0/jakarta.servlet-api-5.0.0.jar ...
	[SUCCESSFUL ] jakarta.servlet#jakarta.servlet-api;5.0.0!jakarta.servlet-api.jar (71ms)
downloading https://repo1.maven.org/maven2/javax/servlet/javax.servlet-api/4.0.1/javax.servlet-api-4.0.1.jar ...
	[SUCCESSFUL ] javax.servlet#javax.servlet-api;4.0.1!javax.servlet-api.jar (59ms)
downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...
	[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (56ms)
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.13/4.0.0/spark-token-provider-kafka-0-10_2.13-4.0.0.jar ...
	[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0!spark-token-provider-kafka-0-10_2.13.jar (62ms)
downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.9.0/kafka-clients-3.9.0.jar ...
	[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.9.0!kafka-clients.jar (591ms)
downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
	[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (62ms)
downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.12.0/commons-pool2-2.12.0.jar ...
	[SUCCESSFUL ] org.apache.commons#commons-pool2;2.12.0!commons-pool2.jar (69ms)
downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.4.1/hadoop-client-runtime-3.4.1.jar ...
	[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.4.1!hadoop-client-runtime.jar (1656ms)
downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
	[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (92ms)
downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.7/snappy-java-1.1.10.7.jar ...
	[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.7!snappy-java.jar(bundle) (179ms)
downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.16/slf4j-api-2.0.16.jar ...
	[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.16!slf4j-api.jar (63ms)
downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.4.1/hadoop-client-api-3.4.1.jar ...
	[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.4.1!hadoop-client-api.jar (1050ms)
downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.10/xz-1.10.jar ...
	[SUCCESSFUL ] org.tukaani#xz;1.10!xz.jar (63ms)
:: resolution report :: resolve 7444ms :: artifacts dl 7764ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	jakarta.servlet#jakarta.servlet-api;5.0.0 from central in [default]
	javax.servlet#javax.servlet-api;4.0.1 from central in [default]
	org.apache.commons#commons-pool2;2.12.0 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.4.1 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.4.1 from central in [default]
	org.apache.iceberg#iceberg-spark-runtime-4.0_2.13;1.10.0 from central in [default]
	org.apache.kafka#kafka-clients;3.9.0 from central in [default]
	org.apache.spark#spark-avro_2.13;4.0.0 from central in [default]
	org.apache.spark#spark-connect_2.13;4.0.0 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.13;4.0.0 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.0 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 from central in [default]
	org.slf4j#slf4j-api;2.0.16 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.tukaani#xz;1.10 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.7 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   18  |   18  |   18  |   0   ||   18  |   18  |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-b54493f2-4165-4bb1-99d9-e8172b1e5bee
	confs: [default]
	18 artifacts copied, 0 already retrieved (125479kB/112ms)
25/09/28 05:01:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
25/09/28 05:01:26 INFO SparkConnectServer: Starting Spark session.
25/09/28 05:01:28 INFO SparkContext: Running Spark version 4.0.0
25/09/28 05:01:28 INFO SparkContext: OS info Linux, 6.8.0-78-generic, amd64
25/09/28 05:01:28 INFO SparkContext: Java version 17.0.15
25/09/28 05:01:28 INFO ResourceUtils: ==============================================================
25/09/28 05:01:28 INFO ResourceUtils: No custom resources configured for spark.driver.
25/09/28 05:01:28 INFO ResourceUtils: ==============================================================
25/09/28 05:01:28 INFO SparkContext: Submitted application: Spark Connect server
25/09/28 05:01:28 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/09/28 05:01:28 INFO ResourceProfile: Limiting resource is cpu
25/09/28 05:01:28 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/09/28 05:01:28 INFO SecurityManager: Changing view acls to: spark
25/09/28 05:01:28 INFO SecurityManager: Changing modify acls to: spark
25/09/28 05:01:28 INFO SecurityManager: Changing view acls groups to: spark
25/09/28 05:01:28 INFO SecurityManager: Changing modify acls groups to: spark
25/09/28 05:01:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY; RPC SSL disabled
25/09/28 05:01:28 INFO Utils: Successfully started service 'sparkDriver' on port 46357.
25/09/28 05:01:28 INFO SparkEnv: Registering MapOutputTracker
25/09/28 05:01:28 INFO SparkEnv: Registering BlockManagerMaster
25/09/28 05:01:28 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/09/28 05:01:28 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/09/28 05:01:28 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/09/28 05:01:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-866fe250-1bb4-4409-bcfb-1bf36caac944
25/09/28 05:01:28 INFO SparkEnv: Registering OutputCommitCoordinator
25/09/28 05:01:29 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/09/28 05:01:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/09/28 05:01:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2.5.2/jars/org.apache.spark_spark-connect_2.13-4.0.0.jar at spark://f6f0f97b91c5:46357/jars/org.apache.spark_spark-connect_2.13-4.0.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2.5.2/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-4.0.0.jar at spark://f6f0f97b91c5:46357/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-4.0.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2.5.2/jars/org.apache.spark_spark-avro_2.13-4.0.0.jar at spark://f6f0f97b91c5:46357/jars/org.apache.spark_spark-avro_2.13-4.0.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2.5.2/jars/org.apache.iceberg_iceberg-spark-runtime-4.0_2.13-1.10.0.jar at spark://f6f0f97b91c5:46357/jars/org.apache.iceberg_iceberg-spark-runtime-4.0_2.13-1.10.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2.5.2/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.2.0.jar at spark://f6f0f97b91c5:46357/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.2.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2.5.2/jars/jakarta.servlet_jakarta.servlet-api-5.0.0.jar at spark://f6f0f97b91c5:46357/jars/jakarta.servlet_jakarta.servlet-api-5.0.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2.5.2/jars/javax.servlet_javax.servlet-api-4.0.1.jar at spark://f6f0f97b91c5:46357/jars/javax.servlet_javax.servlet-api-4.0.1.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2.5.2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://f6f0f97b91c5:46357/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2.5.2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-4.0.0.jar at spark://f6f0f97b91c5:46357/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-4.0.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2.5.2/jars/org.apache.kafka_kafka-clients-3.9.0.jar at spark://f6f0f97b91c5:46357/jars/org.apache.kafka_kafka-clients-3.9.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2.5.2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://f6f0f97b91c5:46357/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2.5.2/jars/org.apache.commons_commons-pool2-2.12.0.jar at spark://f6f0f97b91c5:46357/jars/org.apache.commons_commons-pool2-2.12.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2.5.2/jars/org.apache.hadoop_hadoop-client-runtime-3.4.1.jar at spark://f6f0f97b91c5:46357/jars/org.apache.hadoop_hadoop-client-runtime-3.4.1.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2.5.2/jars/org.lz4_lz4-java-1.8.0.jar at spark://f6f0f97b91c5:46357/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2.5.2/jars/org.xerial.snappy_snappy-java-1.1.10.7.jar at spark://f6f0f97b91c5:46357/jars/org.xerial.snappy_snappy-java-1.1.10.7.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2.5.2/jars/org.slf4j_slf4j-api-2.0.16.jar at spark://f6f0f97b91c5:46357/jars/org.slf4j_slf4j-api-2.0.16.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2.5.2/jars/org.apache.hadoop_hadoop-client-api-3.4.1.jar at spark://f6f0f97b91c5:46357/jars/org.apache.hadoop_hadoop-client-api-3.4.1.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2.5.2/jars/org.tukaani_xz-1.10.jar at spark://f6f0f97b91c5:46357/jars/org.tukaani_xz-1.10.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2.5.2/jars/org.apache.spark_spark-connect_2.13-4.0.0.jar at spark://f6f0f97b91c5:46357/files/org.apache.spark_spark-connect_2.13-4.0.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2.5.2/jars/org.apache.spark_spark-connect_2.13-4.0.0.jar to /tmp/spark-6fab2261-8435-4b8d-a530-7ac643d7e256/userFiles-022f9468-42c8-4c96-9b1d-961b792fbad7/org.apache.spark_spark-connect_2.13-4.0.0.jar
25/09/28 05:01:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2.5.2/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-4.0.0.jar at spark://f6f0f97b91c5:46357/files/org.apache.spark_spark-sql-kafka-0-10_2.13-4.0.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2.5.2/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-4.0.0.jar to /tmp/spark-6fab2261-8435-4b8d-a530-7ac643d7e256/userFiles-022f9468-42c8-4c96-9b1d-961b792fbad7/org.apache.spark_spark-sql-kafka-0-10_2.13-4.0.0.jar
25/09/28 05:01:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2.5.2/jars/org.apache.spark_spark-avro_2.13-4.0.0.jar at spark://f6f0f97b91c5:46357/files/org.apache.spark_spark-avro_2.13-4.0.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2.5.2/jars/org.apache.spark_spark-avro_2.13-4.0.0.jar to /tmp/spark-6fab2261-8435-4b8d-a530-7ac643d7e256/userFiles-022f9468-42c8-4c96-9b1d-961b792fbad7/org.apache.spark_spark-avro_2.13-4.0.0.jar
25/09/28 05:01:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2.5.2/jars/org.apache.iceberg_iceberg-spark-runtime-4.0_2.13-1.10.0.jar at spark://f6f0f97b91c5:46357/files/org.apache.iceberg_iceberg-spark-runtime-4.0_2.13-1.10.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2.5.2/jars/org.apache.iceberg_iceberg-spark-runtime-4.0_2.13-1.10.0.jar to /tmp/spark-6fab2261-8435-4b8d-a530-7ac643d7e256/userFiles-022f9468-42c8-4c96-9b1d-961b792fbad7/org.apache.iceberg_iceberg-spark-runtime-4.0_2.13-1.10.0.jar
25/09/28 05:01:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2.5.2/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.2.0.jar at spark://f6f0f97b91c5:46357/files/org.scala-lang.modules_scala-parallel-collections_2.13-1.2.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2.5.2/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.2.0.jar to /tmp/spark-6fab2261-8435-4b8d-a530-7ac643d7e256/userFiles-022f9468-42c8-4c96-9b1d-961b792fbad7/org.scala-lang.modules_scala-parallel-collections_2.13-1.2.0.jar
25/09/28 05:01:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2.5.2/jars/jakarta.servlet_jakarta.servlet-api-5.0.0.jar at spark://f6f0f97b91c5:46357/files/jakarta.servlet_jakarta.servlet-api-5.0.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2.5.2/jars/jakarta.servlet_jakarta.servlet-api-5.0.0.jar to /tmp/spark-6fab2261-8435-4b8d-a530-7ac643d7e256/userFiles-022f9468-42c8-4c96-9b1d-961b792fbad7/jakarta.servlet_jakarta.servlet-api-5.0.0.jar
25/09/28 05:01:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2.5.2/jars/javax.servlet_javax.servlet-api-4.0.1.jar at spark://f6f0f97b91c5:46357/files/javax.servlet_javax.servlet-api-4.0.1.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2.5.2/jars/javax.servlet_javax.servlet-api-4.0.1.jar to /tmp/spark-6fab2261-8435-4b8d-a530-7ac643d7e256/userFiles-022f9468-42c8-4c96-9b1d-961b792fbad7/javax.servlet_javax.servlet-api-4.0.1.jar
25/09/28 05:01:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2.5.2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://f6f0f97b91c5:46357/files/org.spark-project.spark_unused-1.0.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2.5.2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-6fab2261-8435-4b8d-a530-7ac643d7e256/userFiles-022f9468-42c8-4c96-9b1d-961b792fbad7/org.spark-project.spark_unused-1.0.0.jar
25/09/28 05:01:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2.5.2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-4.0.0.jar at spark://f6f0f97b91c5:46357/files/org.apache.spark_spark-token-provider-kafka-0-10_2.13-4.0.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2.5.2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-4.0.0.jar to /tmp/spark-6fab2261-8435-4b8d-a530-7ac643d7e256/userFiles-022f9468-42c8-4c96-9b1d-961b792fbad7/org.apache.spark_spark-token-provider-kafka-0-10_2.13-4.0.0.jar
25/09/28 05:01:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2.5.2/jars/org.apache.kafka_kafka-clients-3.9.0.jar at spark://f6f0f97b91c5:46357/files/org.apache.kafka_kafka-clients-3.9.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2.5.2/jars/org.apache.kafka_kafka-clients-3.9.0.jar to /tmp/spark-6fab2261-8435-4b8d-a530-7ac643d7e256/userFiles-022f9468-42c8-4c96-9b1d-961b792fbad7/org.apache.kafka_kafka-clients-3.9.0.jar
25/09/28 05:01:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2.5.2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://f6f0f97b91c5:46357/files/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2.5.2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-6fab2261-8435-4b8d-a530-7ac643d7e256/userFiles-022f9468-42c8-4c96-9b1d-961b792fbad7/com.google.code.findbugs_jsr305-3.0.0.jar
25/09/28 05:01:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2.5.2/jars/org.apache.commons_commons-pool2-2.12.0.jar at spark://f6f0f97b91c5:46357/files/org.apache.commons_commons-pool2-2.12.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2.5.2/jars/org.apache.commons_commons-pool2-2.12.0.jar to /tmp/spark-6fab2261-8435-4b8d-a530-7ac643d7e256/userFiles-022f9468-42c8-4c96-9b1d-961b792fbad7/org.apache.commons_commons-pool2-2.12.0.jar
25/09/28 05:01:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2.5.2/jars/org.apache.hadoop_hadoop-client-runtime-3.4.1.jar at spark://f6f0f97b91c5:46357/files/org.apache.hadoop_hadoop-client-runtime-3.4.1.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2.5.2/jars/org.apache.hadoop_hadoop-client-runtime-3.4.1.jar to /tmp/spark-6fab2261-8435-4b8d-a530-7ac643d7e256/userFiles-022f9468-42c8-4c96-9b1d-961b792fbad7/org.apache.hadoop_hadoop-client-runtime-3.4.1.jar
25/09/28 05:01:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2.5.2/jars/org.lz4_lz4-java-1.8.0.jar at spark://f6f0f97b91c5:46357/files/org.lz4_lz4-java-1.8.0.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2.5.2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-6fab2261-8435-4b8d-a530-7ac643d7e256/userFiles-022f9468-42c8-4c96-9b1d-961b792fbad7/org.lz4_lz4-java-1.8.0.jar
25/09/28 05:01:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2.5.2/jars/org.xerial.snappy_snappy-java-1.1.10.7.jar at spark://f6f0f97b91c5:46357/files/org.xerial.snappy_snappy-java-1.1.10.7.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2.5.2/jars/org.xerial.snappy_snappy-java-1.1.10.7.jar to /tmp/spark-6fab2261-8435-4b8d-a530-7ac643d7e256/userFiles-022f9468-42c8-4c96-9b1d-961b792fbad7/org.xerial.snappy_snappy-java-1.1.10.7.jar
25/09/28 05:01:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2.5.2/jars/org.slf4j_slf4j-api-2.0.16.jar at spark://f6f0f97b91c5:46357/files/org.slf4j_slf4j-api-2.0.16.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2.5.2/jars/org.slf4j_slf4j-api-2.0.16.jar to /tmp/spark-6fab2261-8435-4b8d-a530-7ac643d7e256/userFiles-022f9468-42c8-4c96-9b1d-961b792fbad7/org.slf4j_slf4j-api-2.0.16.jar
25/09/28 05:01:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2.5.2/jars/org.apache.hadoop_hadoop-client-api-3.4.1.jar at spark://f6f0f97b91c5:46357/files/org.apache.hadoop_hadoop-client-api-3.4.1.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2.5.2/jars/org.apache.hadoop_hadoop-client-api-3.4.1.jar to /tmp/spark-6fab2261-8435-4b8d-a530-7ac643d7e256/userFiles-022f9468-42c8-4c96-9b1d-961b792fbad7/org.apache.hadoop_hadoop-client-api-3.4.1.jar
25/09/28 05:01:29 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2.5.2/jars/org.tukaani_xz-1.10.jar at spark://f6f0f97b91c5:46357/files/org.tukaani_xz-1.10.jar with timestamp 1759035688317
25/09/28 05:01:29 INFO Utils: Copying /opt/bitnami/spark/.ivy2.5.2/jars/org.tukaani_xz-1.10.jar to /tmp/spark-6fab2261-8435-4b8d-a530-7ac643d7e256/userFiles-022f9468-42c8-4c96-9b1d-961b792fbad7/org.tukaani_xz-1.10.jar
25/09/28 05:01:29 INFO SecurityManager: Changing view acls to: spark
25/09/28 05:01:29 INFO SecurityManager: Changing modify acls to: spark
25/09/28 05:01:29 INFO SecurityManager: Changing view acls groups to: spark
25/09/28 05:01:29 INFO SecurityManager: Changing modify acls groups to: spark
25/09/28 05:01:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY; RPC SSL disabled
25/09/28 05:01:29 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
25/09/28 05:01:29 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 32 ms (0 ms spent in bootstraps)
25/09/28 05:01:29 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250928050129-0001
25/09/28 05:01:29 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250928050129-0001/0 on worker-20250928044627-172.18.0.3-43365 (172.18.0.3:43365) with 6 core(s)
25/09/28 05:01:29 INFO StandaloneSchedulerBackend: Granted executor ID app-20250928050129-0001/0 on hostPort 172.18.0.3:43365 with 6 core(s), 1024.0 MiB RAM
25/09/28 05:01:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42483.
25/09/28 05:01:29 INFO NettyBlockTransferService: Server created on f6f0f97b91c5:42483
25/09/28 05:01:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/09/28 05:01:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f6f0f97b91c5, 42483, None)
25/09/28 05:01:29 INFO BlockManagerMasterEndpoint: Registering block manager f6f0f97b91c5:42483 with 434.4 MiB RAM, BlockManagerId(driver, f6f0f97b91c5, 42483, None)
25/09/28 05:01:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f6f0f97b91c5, 42483, None)
25/09/28 05:01:29 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f6f0f97b91c5, 42483, None)
25/09/28 05:01:29 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250928050129-0001/0 is now RUNNING
25/09/28 05:01:30 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
25/09/28 05:01:32 INFO Utils: Successfully started service 'org.apache.spark.sql.connect.service.SparkConnectService$' on port 15002.
25/09/28 05:01:32 INFO SparkConnectServer: Spark Connect server started at: 0:0:0:0:0:0:0:0:15002
25/09/28 05:01:33 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:58410) with ID 0, ResourceProfileId 0
25/09/28 05:01:34 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.3:35201 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.3, 35201, None)
25/09/28 05:01:50 INFO SparkConnectSessionManager: Starting thread for cleanup of expired sessions every 30000 ms
25/09/28 05:01:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/09/28 05:01:50 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
25/09/28 05:01:50 INFO SessionHolder: Session with userId: javier15v and sessionId: 4e525490-5ac5-4435-91a5-995f5937283c accessed,time 1759035710479 ms.
25/09/28 05:01:50 INFO SessionHolder: Session with userId: javier15v and sessionId: 4e525490-5ac5-4435-91a5-995f5937283c accessed,time 1759035710487 ms.
25/09/28 05:01:50 ERROR ErrorUtils: Spark Connect RPC error during: config. UserId: javier15v. SessionId: 4e525490-5ac5-4435-91a5-995f5937283c.
org.apache.spark.SparkSQLException: [INVALID_HANDLE.SESSION_CHANGED] The handle 4e525490-5ac5-4435-91a5-995f5937283c is invalid. The existing Spark server driver instance has restarted. Please reconnect. SQLSTATE: HY000
	at org.apache.spark.sql.connect.service.SparkConnectSessionManager.validateSessionId(SparkConnectSessionManager.scala:62)
	at org.apache.spark.sql.connect.service.SparkConnectSessionManager.$anonfun$getOrCreateIsolatedSession$2(SparkConnectSessionManager.scala:81)
	at org.apache.spark.sql.connect.service.SparkConnectSessionManager.$anonfun$getOrCreateIsolatedSession$2$adapted(SparkConnectSessionManager.scala:80)
	at scala.Option.foreach(Option.scala:437)
	at org.apache.spark.sql.connect.service.SparkConnectSessionManager.getOrCreateIsolatedSession(SparkConnectSessionManager.scala:80)
	at org.apache.spark.sql.connect.service.SparkConnectService$.getOrCreateIsolatedSession(SparkConnectService.scala:328)
	at org.apache.spark.sql.connect.service.SparkConnectConfigHandler.handle(SparkConnectConfigHandler.scala:43)
	at org.apache.spark.sql.connect.service.SparkConnectService.config(SparkConnectService.scala:120)
	at org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:911)
	at org.sparkproject.connect.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)
	at org.sparkproject.connect.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)
	at org.sparkproject.connect.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)
	at org.sparkproject.connect.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)
	at org.sparkproject.connect.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
25/09/28 05:02:06 INFO SessionHolder: Session with userId: javier15v and sessionId: 5b3d6a0b-6889-4cfb-8aa9-7128423b6841 accessed,time 1759035726881 ms.
25/09/28 05:02:06 INFO SessionHolder: Session with userId: javier15v and sessionId: 5b3d6a0b-6889-4cfb-8aa9-7128423b6841 accessed,time 1759035726925 ms.
25/09/28 05:02:17 INFO SessionHolder: Session with userId: javier15v and sessionId: 5b3d6a0b-6889-4cfb-8aa9-7128423b6841 accessed,time 1759035737355 ms.
25/09/28 05:02:20 INFO SessionHolder: Session with userId: javier15v and sessionId: 5b3d6a0b-6889-4cfb-8aa9-7128423b6841 accessed,time 1759035740133 ms.
25/09/28 05:02:20 INFO SessionHolder: Session with userId: javier15v and sessionId: 5b3d6a0b-6889-4cfb-8aa9-7128423b6841 accessed,time 1759035740278 ms.
25/09/28 05:02:20 INFO SparkConnectExecutionManager: ExecuteHolder ExecuteKey(javier15v,5b3d6a0b-6889-4cfb-8aa9-7128423b6841,21de5e09-7ffe-428e-b7c0-7cf16a26ff36) is created.
25/09/28 05:02:20 INFO SparkConnectExecutionManager: Starting thread for cleanup of abandoned executions every 30000 ms
25/09/28 05:02:20 INFO ExecuteGrpcResponseSender: Starting for opId=21de5e09-7ffe-428e-b7c0-7cf16a26ff36, reattachable=true, lastConsumedStreamIndex=0
25/09/28 05:02:21 INFO CodeGenerator: Code generated in 350.10131 ms
25/09/28 05:02:21 INFO CodeGenerator: Code generated in 50.578371 ms
25/09/28 05:02:21 INFO DAGScheduler: Asked to cancel jobs with tag SparkConnect_OperationTag_User_javier15v_Session_5b3d6a0b-6889-4cfb-8aa9-7128423b6841_Operation_21de5e09-7ffe-428e-b7c0-7cf16a26ff36
25/09/28 05:02:21 INFO SessionHolder: Session with userId: javier15v and sessionId: 5b3d6a0b-6889-4cfb-8aa9-7128423b6841 accessed,time 1759035741962 ms.
25/09/28 05:02:21 INFO ErrorUtils: Spark Connect error during: execute. UserId: javier15v. SessionId: 5b3d6a0b-6889-4cfb-8aa9-7128423b6841.
org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `headers` cannot be resolved. Did you mean one of the following? [`key`, `value`, `offset`, `topic`, `partition`]. SQLSTATE: 42703;
'Project [key#1, value#2, 'headers, partition#4, offset#5L, timestamp#6]
+- Relation [key#1,value#2,topic#3,partition#4,offset#5L,timestamp#6,timestampType#7] KafkaRelation(strategy=Subscribe[erp_avro.erp.orders], start=EarliestOffsetRangeLimit, end=LatestOffsetRangeLimit)

	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:249)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:249)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformShowString(SparkConnectPlanner.scala:306)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.$anonfun$transformRelation$1(SparkConnectPlanner.scala:150)
	at org.apache.spark.sql.connect.service.SessionHolder.$anonfun$usePlanCache$3(SessionHolder.scala:477)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.connect.service.SessionHolder.usePlanCache(SessionHolder.scala:476)
	at org.apache.spark.sql.connect.planner.SparkConnectPlanner.transformRelation(SparkConnectPlanner.scala:147)
	at org.apache.spark.sql.connect.execution.SparkConnectPlanExecution.handlePlan(SparkConnectPlanExecution.scala:74)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handlePlan(ExecuteThreadRunner.scala:314)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:225)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:196)
	at org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:341)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:341)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
	at org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:186)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:102)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
	at org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:340)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:196)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:125)
	at org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:347)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)
		at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)
		at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:249)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)
		at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:249)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 28 more
25/09/28 05:02:21 INFO ExecuteGrpcResponseSender: Stream finished for opId=21de5e09-7ffe-428e-b7c0-7cf16a26ff36, sent all responses up to last index 0. totalTime=1518.130722 ms waitingForResults=1516.338214 ms waitingForSend=0.0 ms
25/09/28 05:02:21 INFO SessionHolder: Session with userId: javier15v and sessionId: 5b3d6a0b-6889-4cfb-8aa9-7128423b6841 accessed,time 1759035741980 ms.
25/09/28 05:02:21 INFO SessionHolder: Session with userId: javier15v and sessionId: 5b3d6a0b-6889-4cfb-8aa9-7128423b6841 accessed,time 1759035741982 ms.
25/09/28 05:02:21 INFO SparkConnectExecutionManager: ExecuteHolder ExecuteKey(javier15v,5b3d6a0b-6889-4cfb-8aa9-7128423b6841,21de5e09-7ffe-428e-b7c0-7cf16a26ff36) is removed.
25/09/28 05:02:21 INFO ExecuteResponseObserver: Release all for opId=21de5e09-7ffe-428e-b7c0-7cf16a26ff36. Execution stats: total=CachedSize(0,0) autoRemoved=CachedSize(0,0) cachedUntilConsumed=CachedSize(0,0) cachedUntilProduced=CachedSize(0,0) maxCachedUntilConsumed=CachedSize(0,0) maxCachedUntilProduced=CachedSize(0,0)
25/09/28 05:02:22 INFO SessionHolder: Session with userId: javier15v and sessionId: 5b3d6a0b-6889-4cfb-8aa9-7128423b6841 accessed,time 1759035742013 ms.
